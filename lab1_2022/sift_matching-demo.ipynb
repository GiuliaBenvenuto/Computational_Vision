{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SURNAME: Benvenuto NAME: Giulia\n",
    "#### **I cleared all the outputs because otherwise the size of the notebook was too big to be uploaded on aulaweb.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT - a demo\n",
    "\n",
    "The objective of this activity is to critically analyse the results you obtain by running the code and learn about the potential of SIFT descriptors for feature matching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time with OpenCV? \n",
    "\n",
    "So far we have used skimage. OpenCV has a wider range of Computational Vision pre-implemented functionalities.\n",
    "\n",
    "Have a look at <a href=\"https://docs.opencv.org/master/d9/df8/tutorial_root.html\" > online tutorials</a>\n",
    "\n",
    "First, let us import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SIFT computation and visualization\n",
    "### Black & White image\n",
    "\n",
    "If you want to know something more click on the <a http=\"https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html\"> SIFT OpenCV tutorial </a> \n",
    "\n",
    "We'll try out with an intensity image first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images/Rubik1.pgm')\n",
    "gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "img_sift = None\n",
    "\n",
    "sift = cv.SIFT_create()\n",
    "kp = sift.detect(gray,None)\n",
    "img_sift = cv.drawKeypoints(gray,kp,img_sift, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) #try and delete the flag\n",
    "cv.imwrite('sift_keypoints.jpg', img_sift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50, 100)) \n",
    "fig.add_subplot(1,2,1)\n",
    "#plt.imshow(cv.cvtColor(gray, cv.COLOR_BGR2RGB))\n",
    "plt.imshow(img)\n",
    "fig.add_subplot(1,2,2)\n",
    "#plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "plt.imshow(img_sift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you comment the output you just obtain? Do the detected features make sense?**\n",
    "\n",
    "The provided code implements the SIFT (Scale-Invariant Feature Transform) algorithm which with the keypoints are detected in a grayscale image.\n",
    "\n",
    "OpenCV provides the function that we used *cv.drawKeyPoints()* which draws the circles on the locations of keypoints which correspond to interesting points in the image that are distinctive and can be reliably detected across different scales and orientations. If we pass the flag *cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS* to it, it will draw a circle with size of keypoint and it will even show its orientation.\n",
    "\n",
    "In output, in fact, we get the original image where all the keypoints that have been found, the detected features seem to have sense. Most of them are located in the dotted background of the Rubik's cube where there are a lot of intensity changes and where there are regions of high contrast. Moreover inside each circle there is a line that indicates the orientation of the variation. \n",
    "\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color image\n",
    "We try now with a color image. Notice that OpenCV uses BGR as a default color space, while the visualization functions don't (thus we need to convert BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER EXAMPLE\n",
    "img = cv.imread('imageset/BackgroundChange/09601.jpg')\n",
    "gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "img_sift = None\n",
    "\n",
    "sift = cv.SIFT_create()\n",
    "kp = sift.detect(gray,None)\n",
    "img_sift=cv.drawKeypoints(img,kp,img_sift, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) #try and delete the flag\n",
    "\n",
    "fig = plt.figure(figsize=(50, 100)) \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "#plt.imshow(img)\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img_sift, cv.COLOR_BGR2RGB))\n",
    "#plt.imshow(img_sift)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you comment the output you just obtain? Do the detected features make sense?**\n",
    "\n",
    "As in the b&w image, also in this case I think that the features have some sense: they are located in the points of the scene where we have a bigger number of objects / elements / changes / variations in the image. The white background has a small number of features and all of them close to the points where \"something happen\", in the uniform tint pieces of the scene no keypoints are detected. The cereal box is full of colored text elements and pictorial elements so here we have most of the keypoints detedted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image matching with SIFT features\n",
    "\n",
    "We now compute matches between SIFT features from an image pair. In the example we notice a background, illumination and scale change\n",
    "\n",
    "### 2.1 Detection first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/BackgroundChange/11201.jpg')\n",
    "img2 = cv.imread('imageset/BackgroundChange/18301.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Matching\n",
    "The  following feature matching will follow two steps:\n",
    "1. Brute force matching\n",
    "2. Lowe's filtering \n",
    "\n",
    "Brute force matching is a simple: take the featue description of each feature in one image and calculate it's distance from every feature in the other image. Lowe filtering is a thresholding technique to eliminate matches with high distances. \n",
    "\n",
    "If you have time, you may check other parameters of BFMatcher. You may try them and see how the results change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the above with other images pairs from the sets provided. In particular, try image pairs from  \n",
    "1. Imageset/2DMovement \n",
    "2. Imageset/ColorChanges\n",
    "3. Imageset/Random\n",
    "\n",
    "Observe the success and failure of the two methods against different types of variations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv.NORM_L1 Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv.BFMatcher(cv.NORM_L1, crossCheck=False)\n",
    "matches = bf.knnMatch(des1, des2, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Movements\n",
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/2DMovements/06801.jpg')\n",
    "img2 = cv.imread('imageset/2DMovements/09401.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)\n",
    "\n",
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ADD YOUR COMMENTS HERE </b>\n",
    "\n",
    "These two images are different because the cereal box is rotated with a 2D movement. The matching procedure is less precise than the case before in which the two images were more similar, despite this the matching procedure is still good even if we can see some errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/2DMovements/09401.jpg')\n",
    "img2 = cv.imread('imageset/2DMovements/10201.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)\n",
    "\n",
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ADD YOUR COMMENTS HERE </b>\n",
    "\n",
    "In this case we have a small 2D rotation of the cereal box in fact, the matching procedure is really precise and accurate. Most of the connected features seem to be correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/3DMovements/18301.jpg')\n",
    "img2 = cv.imread('imageset/3DMovements/25301.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)\n",
    "\n",
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ADD YOUR COMMENTS HERE </b>\n",
    "\n",
    "In this case we have a 3D rotation of the cereal box which has a really different position, the overlapped portion of the scene in the two images are fewer. The algorithm in this case is able to find only a few matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/BackgroundChange/07701.jpg')\n",
    "img2 = cv.imread('imageset/BackgroundChange/11201.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)\n",
    "\n",
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ADD YOUR COMMENTS HERE </b>\n",
    "\n",
    "If the background of two images changes between feature detection and matching, it can affect the accuracy of the matching results.\n",
    "\n",
    "In general we know that feature matching relies on identifying distinctive and invariant features in the images, such as corners, edges, or blobs, that can be matched across different viewpoints or under different lighting conditions. These features are often located in the foreground of the images, where the objects of interest are, and are less affected by changes in the background.\n",
    "\n",
    "However, if the background changes significantly, it may introduce new features or remove existing ones, making it more difficult to find correspondences between the features detected in the two images.\n",
    "\n",
    "In this case, the beackgrounds of the two images are really different and have a different illumination but the result is still quite good since the ceral box is clearly visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion\n",
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/Occlusion/30401.jpg')\n",
    "img2 = cv.imread('imageset/Occlusion/31101.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)\n",
    "\n",
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ADD YOUR COMMENTS HERE </b>\n",
    "\n",
    "In this case the occluded portion of the second image is not soo big, thus the visibility of the ceral box is not soo nicked and the most significant part of the image, where we have the most of the features is still quite visibile. For all these reasons the feature matching result is still good as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot relevant images\n",
    "img1 = cv.imread('imageset/Occlusion/33501.jpg')\n",
    "img2 = cv.imread('imageset/Occlusion/48901.jpg')\n",
    "\n",
    "# Parallel display of images\n",
    "fig = plt.figure() \n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1, cv.COLOR_BGR2RGB))\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "img1_with_kp = None\n",
    "img2_with_kp = None\n",
    "img1_with_kp = cv.drawKeypoints(img1, kp1, img1_with_kp)\n",
    "img2_with_kp = cv.drawKeypoints(img2, kp2, img2_with_kp)\n",
    "\n",
    "# Converting image1 from BGR to RGB (OpenCV design)\n",
    "img1_with_kp=cv.drawKeypoints(img1, kp1, img1_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_with_kp=cv.drawKeypoints(img2, kp2, img2_with_kp, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv.cvtColor(img1_with_kp, cv.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv.cvtColor(img2_with_kp, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv.BFMatcher()\n",
    "\n",
    "# Testing the top two best matches to increase matching accuracy\n",
    "matches = bf.knnMatch(des1,des2, k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "img3 = None\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,img3,flags=2)\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.imshow(cv.cvtColor(img3, cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> ADD YOUR COMMENTS HERE </b>\n",
    "\n",
    "In this case the occluded portion of the second image is bigger. The second image is visible only patrially. The visibility of the image is compromised and this produces a worse result. Some of the matching features are wrong, in particular those in the bottom of the writing on the box which is not visible in the second image.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
