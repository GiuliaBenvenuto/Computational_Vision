{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SURNAME: Benvenuto NAME: Giulia\n",
    "#### **I cleared all the outputs because otherwise the size of the notebook was too big to be uploaded on aulaweb.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import division # uncomment this if using Python 2.7\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import signal, spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from skimage import data, color, img_as_float, img_as_ubyte, filters, feature, util, io\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature matching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we are going to dive deeper into <i>image matching</i>, specifically on a local approach based on features correspondances. \n",
    "\n",
    "We first consider the simplest situation possible: the image pairs we consider are related by simple transformations, therefore we will make the following choices:\n",
    "1. Feature detector: corners (with the shi-tomasi algorithm)\n",
    "2. Feature description: patches around the corner (size $w\\times w$)\n",
    "3. Matching strategy: affinity matrix with a similarity measure of choice\n",
    "\n",
    "\n",
    "The parameters of every intermediate step, must be specified as input arguments. Try with different distance metrics (e.g. euclidean, correlation, squared euclidean) and with all three image pairs (Rubik, Shrub, Phone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us define the main functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first write a function that extracts patches of image around each corner (w.r.t. the patch size, that is a parameter)\n",
    "* First we need to pad our image, adding a surrounding frame of appropriate width. In this way even border features (like the red one in the drawing) will have their neighbourhood\n",
    "\n",
    "Then, for each corner, \n",
    "* we adjust the corner coordinates with respect to the new padded image I_ext\n",
    "* we extract the size_w X size_w patch surrounding the corner (check the range notation)\n",
    "* we flatten the patch to form a 1D feature vector of size 2*size_w and save it in a list\n",
    "\n",
    "\n",
    "<img src=\"week2_lab_notes.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_descriptor(I, corner_pos, size_w):\n",
    "    \"\"\"Extract square patches around each corner on an input grayscale image. \n",
    "    - I: input RGB image\n",
    "    - corner_pos: list with position of n corners (row,col)\n",
    "    - size_w: (integer) patch side\n",
    "   \"\"\"\n",
    "    \n",
    "    n = len(corner_pos) # Number of features\n",
    "    hw = int(np.floor(size_w/2)) # half size of the patch (useful to center the pat)\n",
    "    I_ext = np.pad(I, hw, 'reflect') # pad the image with a frame of width hw \n",
    "    \n",
    "    # initialize patches list\n",
    "    patches = np.zeros([n,(2*hw+1)**2])\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        \n",
    "        r = corner_pos[i,0]+hw # adjust the row of each corner considering the padding\n",
    "        c = corner_pos[i,1]+hw # do the same for the column \n",
    "        tmp = I_ext[r-hw:r+hw+1, c-hw:c+hw+1] # estract the patch\n",
    "        patches[i,:] = tmp.flatten() #flatten the patch and save it in the patches list\n",
    "        \n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function that computes the feature matching procedure.\n",
    "Assuming we have $m$ features in image 1 and $n$ features in image 2, the affinity matrix will have size $m\\times n$.\n",
    "For each feature in image 1, we will be matching the one in image 2 <i>minimizes</i> the distance.\n",
    "\n",
    "Below you find a simple version of the procedure  \n",
    "1. First compute the affinity matrix \n",
    "2. Then detect the maxima of the affinity matrix\n",
    "3. Derive che corresponding matches\n",
    "4. Return the matches and the affinity matrix\n",
    "\n",
    "**Hint:** First check <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\">`spatial.distance.cdist`</a> from `scipy` module, with Euclidean metric.\n",
    "Then use the formulae below to compute the elements of an affinity matrix (values in the range [0,1]\n",
    "$\\exp^{-d(f_i,g_j)/2\\sigma^2}$ ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_matching_euclidean(sigma, patches1, patches2, corner_pos1, corner_pos2):\n",
    "    \n",
    "    # evaluate the distance among patches [HERE WE ARE JUST USING THE APPEARANCE SIMILARITY]\n",
    "    D = spatial.distance.cdist(patches1, patches2, metric='euclidean')\n",
    "    # compute the affinity matrix using the exponent formulation\n",
    "    E = np.exp(-D//(2*sigma*sigma))\n",
    "    # find the minimum distace\n",
    "    argmaxE_h = np.argmax(E, axis=1) #axis=1 : rows\n",
    "    argmaxE_v = np.argmax(E, axis=0) #axis=0 : column\n",
    "    match = []\n",
    "    \n",
    "    #Take each value on the row and if the max value on the row is the maxvalue also an the correspondent column put it into match\n",
    "    for i, amx in enumerate(argmaxE_h):\n",
    "        if (argmaxE_v[amx] == i):  \n",
    "            match.append(tuple([i, amx]))\n",
    "        \n",
    "              \n",
    "    # plot the matching pairs (match includes the match from corner_pos2 associated with corner_pos1)\n",
    "    #match = corner_pos2[argmaxE,:]\n",
    "    return E, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity matrix computed by using: 'correlation' as distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_matching_correlation(sigma, patches1, patches2, corner_pos1, corner_pos2):\n",
    "    \n",
    "    # evaluate the distance among patches [HERE WE ARE JUST USING THE APPEARANCE SIMILARITY]\n",
    "    D = spatial.distance.cdist(patches1, patches2, metric='correlation')\n",
    "    # compute the affinity matrix using the exponent formulation\n",
    "    E = np.exp(-D//(2*sigma*sigma))\n",
    "    # find the minimum distace\n",
    "    argmaxE_h = np.argmax(E, axis=1) #axis=1 : rows\n",
    "    argmaxE_v = np.argmax(E, axis=0) #axis=0 : column\n",
    "    match = []\n",
    "    \n",
    "    #Take each value on the row and if the max value on the row is the maxvalue also an the correspondent column put it into match\n",
    "    for i, amx in enumerate(argmaxE_h):\n",
    "        if (argmaxE_v[amx] == i):  \n",
    "            match.append(tuple([i, amx]))\n",
    "              \n",
    "    # plot the matching pairs (match includes the match from corner_pos2 associated with corner_pos1)\n",
    "    #match = corner_pos2[argmaxE,:]\n",
    "    return E, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity matrix computed by using: 'sqeuclidean' as distance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_matching_sqeuclidean(sigma, patches1, patches2, corner_pos1, corner_pos2):\n",
    "    \n",
    "    # evaluate the distance among patches [HERE WE ARE JUST USING THE APPEARANCE SIMILARITY]\n",
    "    D = spatial.distance.cdist(patches1, patches2, metric='sqeuclidean')\n",
    "    # compute the affinity matrix using the exponent formulation\n",
    "    E = np.exp(-D//(2*sigma*sigma))\n",
    "    # find the minimum distace\n",
    "    argmaxE_h = np.argmax(E, axis=1) #axis=1 : rows\n",
    "    argmaxE_v = np.argmax(E, axis=0) #axis=0 : column\n",
    "    match = []\n",
    "    \n",
    "    #Take each value on the row and if the max value on the row is the maxvalue also an the correspondent column put it into match\n",
    "    for i, amx in enumerate(argmaxE_h):\n",
    "        if (argmaxE_v[amx] == i):  \n",
    "            match.append(tuple([i, amx]))\n",
    "              \n",
    "    # plot the matching pairs (match includes the match from corner_pos2 associated with corner_pos1)\n",
    "    #match = corner_pos2[argmaxE,:]\n",
    "    return E, match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that plots the images side-by-side and superimposes the features (<b>hint:</b> beware of the offset!) and a line connecting the matched pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_match(match,corner_pos1,corner_pos2,img1, img2):\n",
    "    \"\"\"show match on side-by-side images\"\"\"\n",
    "    \n",
    "    img= np.concatenate([img1,img2],axis=1)\n",
    "    plt.imshow(img, cmap=cm.gist_gray)\n",
    "    \n",
    "    for pair in match:\n",
    "    #for i in range(0, len(corner_pos1)):\n",
    "        #plt.plot([corner_pos1[i,1], match[i,1]+img1.shape[1]], [corner_pos1[i,0], match[i,0]],'y')\n",
    "        plt.plot([corner_pos1[pair[0],1], corner_pos2[pair[1],1]+img1.shape[1]], [corner_pos1[pair[0],0], corner_pos2[pair[1],0]], 'y')\n",
    "    \n",
    "    \n",
    "    plt.scatter(corner_pos1[:,1], corner_pos1[:,0], s=10, c='r')\n",
    "    plt.scatter(corner_pos2[:,1]+img1.shape[1], corner_pos2[:,0], s=20, c='b')\n",
    "    \n",
    "    return img    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us test the feature matching pipeline\n",
    "\n",
    "We start with a simple image pair. We first load them and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "RGBimg1 = io.imread('images/shrub_L.jpg')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/shrub_R.jpg') #(See below)\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    " \n",
    "#PLOT THEM\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=\n",
    "           cm.gist_gray)\n",
    "plt.title('Image 1')\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.title('Image 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us identify the corners by using the shi tomasi algorithm implemented in skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE DETECTION\n",
    "# using the shi-tomasi algorithm, identify the corners in both images\n",
    "\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), num_peaks = 100) # IT MAY BE WORTH ADDING num_peaks=300 to corner_peaks\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2), num_peaks = 100) # IT MAY BE WORTH ADDING num_peaks=300 to corner_peaks\n",
    "\n",
    "# plot the results on both images side by side\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], s=30)\n",
    "plt.title('skimage.feature.corner_peaks result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], s=30, c='r')\n",
    "plt.title('skimage.feature.corner_peaks result');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE DESCRIPTORS\n",
    "# TRY WITH DIFFERENT WINDOW SIZES\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patches2 = patch_descriptor(img2, corners2, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance metrics: \"euclidean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching_euclidean(0.5, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__ here I used the distance metrics \"euclidean\", this means that the Affinity matrix *D* is computed by taking the euclidean distance between *patch1* and *patch2*. Visualizing the affinity matrix as an image we notice that most of the values are equal to zero or really close to it, this means that most of the features are matched in the right way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance metrics: \"correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching_correlation(0.5, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__ here I used the distance metrics \"correlation\". The correlation distance metric measures the similarity between two variables by calculating the distance between their normalized values. In this case, differently from what happend on the previous case we have that by visualizing the affinity matrix as an image it is possible to notice that a lot more value are different from zero, which correspond to a bigger number of features matched in the wrong way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance metrics: \"sqeuclidean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching_sqeuclidean(0.5, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with the other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "RGBimg1 = io.imread('images/Rubik1.pgm')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/Rubik2.pgm') #(See below)\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    " \n",
    "#PLOT THEM\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.title('Image 1')\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.title('Image 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE DETECTION\n",
    "# using the shi-tomasi algorithm, identify the corners in both images\n",
    "\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.1)\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2), threshold_rel=0.1) \n",
    "\n",
    "# plot the results on both images side by side\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], s=30)\n",
    "plt.title('skimage.feature.corner_peaks result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], s=30, c='r')\n",
    "plt.title('skimage.feature.corner_peaks result');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window size = 10\n",
    "- In this case I tried the window size = 10.\n",
    "\n",
    "This means that the patch side is small and this has as consequence more errors because it's more difficult to match patches in which there are less informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE DESCRIPTORS\n",
    "# TRY WITH DIFFERENT WINDOW SIZES\n",
    "patches1 = patch_descriptor(img1, corners1, 10)\n",
    "patches2 = patch_descriptor(img2, corners2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching_euclidean(0.8, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window size = 50\n",
    "- In this case I tried the window size = 50.\n",
    "\n",
    "This means that the patch side is bigger respect to the case before and this has as consequence less errors because it's less difficult to match patches in which there are more informations that help to distinguish the portion of the image that is sourrounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE DESCRIPTORS\n",
    "# TRY WITH DIFFERENT WINDOW SIZES\n",
    "patches1 = patch_descriptor(img1, corners1, 50)\n",
    "patches2 = patch_descriptor(img2, corners2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching_euclidean(0.8, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "There are a number of improvement you could consider at some point. Below you find a mandatory improvement and some optional ones\n",
    "1.  The function spectral_matching does not compute the maxima in the correct way; <i>correct the function</i> so that the identified feature pairs are maxima of both rows and colums . \n",
    "\n",
    "Optionally you may also have a look at the following\n",
    "1. spectral_matching: you may add a threshold on the distance, setting to 0 the affinity matrix entries below a threshold(see theory)\n",
    "2. In the affinity matrix computation, one could also include an evaluation on the position of\n",
    "features (close features should be favoured if we have a prior on image similarity). Again, see theory\n",
    "2. Corners on the border of the image should be discarded as they tend to be less reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral matching corrected\n",
    "Now the *spectral_matching2* function check maxima of boths rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_matching2(sigma, patches1, patches2,corner_pos1,corner_pos2):\n",
    "    \n",
    "    # evaluate the distance among patches\n",
    "    D = spatial.distance.cdist(patches1,patches2, metric='euclidean')\n",
    "    # compute the affinity matrix using the exponent formulation\n",
    "    E = np.exp(-D/(2*sigma*sigma))\n",
    "    # find the minimum distace\n",
    "    argmaxR = np.argmax(E, axis=1)\n",
    "    argmaxC = np.argmax(E, axis=0)\n",
    "    \n",
    "    match = []\n",
    "    for row, maxC in enumerate(argmaxR):\n",
    "        if argmaxC[maxC] == row:\n",
    "            match.append([row, corner_pos2[maxC, 0], corner_pos2[maxC, 1]])\n",
    "    # plot the matching pairs (match includes the match from corner_pos2 associated with corner_pos1)\n",
    "    #match = corner_pos2[argmaxE,:]\n",
    "    \n",
    "    return E, np.array(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_match2(match, corner_pos1, corner_pos2, img1, img2):\n",
    "    \n",
    "    # match now has information on also which corner1 the match refer to, because some may not have a match anymore\n",
    "    \n",
    "    img = np.concatenate([img1, img2], axis=1)\n",
    "    plt.imshow(img, cmap=cm.gist_gray)\n",
    "    \n",
    "    for i in range(match.shape[0]):\n",
    "        plt.plot([corner_pos1[ match[i, 0], 1 ], match[i, 2] + img1.shape[1]], [corner_pos1[ match[i, 0], 0], match[i, 1]], 'y')\n",
    "    \n",
    "    plt.scatter(corner_pos1[:,1], corner_pos1[:,0], s=10)\n",
    "    plt.scatter(corner_pos2[:,1]+img1.shape[1], corner_pos2[:,0], s=20, c='r')\n",
    "    \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "RGBimg1 = io.imread('images/shrub_L.jpg')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/shrub_R.jpg')\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    " \n",
    "#PLOT THEM\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.title('Image 1')\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.title('Image 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.08)\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2), threshold_rel=0.08)\n",
    "\n",
    "# plot the results on both images side by side\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], s=30)\n",
    "plt.title('skimage.feature.corner_peaks result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], s=30, c='r')\n",
    "plt.title('skimage.feature.corner_peaks result');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patches2 = patch_descriptor(img2, corners2, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching2(0.5, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match2(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__ with the corrected function *spectral_matching2* the result is better. Now the feature matching is way more precise. In fact if we consider the image with the road sign, respect to the first case with the wrong *spectral_matching*, now the result is much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "RGBimg1 = io.imread('images/phone1.png')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/phone2.png')\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    " \n",
    "#PLOT THEM\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.title('Image 1')\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.title('Image 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.1)\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2), threshold_rel=0.1)\n",
    "\n",
    "# plot the results on both images side by side\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(RGBimg1, cmap=cm.gist_gray)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], s=30)\n",
    "plt.title('skimage.feature.corner_peaks result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(RGBimg2, cmap=cm.gist_gray)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], s=30, c='r')\n",
    "plt.title('skimage.feature.corner_peaks result');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patches2 = patch_descriptor(img2, corners2, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE MATCHING\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching2(0.5, patches1, patches2, corners1, corners2)\n",
    "match_euclidean = show_match2(match,corners1,corners2,img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO HAVE A LOOK AT THE AFFINITY MATRIX\n",
    "plt.imshow(D) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Here are some experiments you should carry out. Add code snippets and comments below. Conclude with a final discussion section.\n",
    "1. Debug n. 0 is to use the same image for img1 and img2. Answer before proceeding: what do you expect to find? Check the Affinity Matrix\n",
    "2. How to proceed if you want to obtain fewer corners?\n",
    "3. Now try out with two different images of the same object, again analyse the matches as well as the affinity matrix. Try out different sigma values: are the results in line with your expectations?  \n",
    "4. now  \"break\" the simple matching procedure by applying appropriate image transformations: use, as inputs, I1 and transformed(I1), where transformed may be:\n",
    "    - small or large translations\n",
    "    - small or large rotations\n",
    "    - small or large zoom in\n",
    "\n",
    "5. To be sure you fully grasp the concepts, you should try out other image pairs (see the Images folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Analysis\n",
    "If we use the same image for img1 and img2 all the corners are matched perfectly because it's like if they're matching with \"themselves\". I expect the affinity matrix square and diagonal with all the values equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "RGBimg1 = io.imread('images/Rubik1.pgm')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/Rubik1.pgm')\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    "\n",
    "# FEATURE DETECTION\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.1)\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2), threshold_rel=0.1)\n",
    "\n",
    "# FEATURE DESCRIPTION\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patches2 = patch_descriptor(img2, corners2, 40)\n",
    "\n",
    "# PLOT THE RESULT\n",
    "plt.figure(figsize=(12,6))\n",
    "D,match = spectral_matching2(0.5, patches1, patches2, corners1, corners2)\n",
    "show_match2(match, corners1, corners2, img1, img2)\n",
    "\n",
    "# AFFINITY MATRIX and IDENTITY MATRIX\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "axs[0].imshow(D)\n",
    "axs[0].set_title(\"Affinity Matrix\")\n",
    "axs[1].imshow(np.eye(D.shape[0]))\n",
    "axs[1].set_title(\"Identity Matrix\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Diagonal of the Affinity Matrix\")\n",
    "print(D.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Analysis\n",
    "If we want less corners we might increase the parameter sigma in the shi-tomasi algorithm, or increase the threshold parameter in the function corner_peaks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigma = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "RGBimg1 = io.imread('images/phone1.png')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/phone2.png')\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    "\n",
    "# FEATURE DETECTION\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1, sigma = 4), threshold_rel=0.1)\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2, sigma = 4), threshold_rel=0.1)\n",
    "\n",
    "# FEATURE DESCRIPTION\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patches2 = patch_descriptor(img2, corners2, 40)\n",
    "\n",
    "# PLOT THE RESULT\n",
    "plt.figure(figsize=(12,6))\n",
    "D, match = spectral_matching2(0.5, patches1, patches2, corners1, corners2)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1, cmap=cm.gist_gray)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], s=30)\n",
    "plt.title('skimage.feature.corner_peaks result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2, cmap=cm.gist_gray)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], s=30, c='r')\n",
    "plt.title('skimage.feature.corner_peaks result');\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,corners2,img1, img2)\n",
    "plt.subplots()\n",
    "plt.imshow(D)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corners 1:\" + str(corners1.shape))\n",
    "print(\"Corners 2:\" + str(corners2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Analysis\n",
    "The affinity matrix is not square anymore and its entry are very small.\n",
    "\n",
    "From the value assigned to the \"sigma\" parameter depend the number of corners detected in fact, in general we have that:\n",
    "- when you increase the sigma value, the Gaussian filter used for smoothing the image will have a larger standard deviation, leading to a more blurred image. This can result in a reduction in the number of detected corners as well as a decrease in their sharpness. \n",
    "- when you decrease the sigma value, the Gaussian filter will have a smaller standard deviation, resulting in a sharper image with more detailed corners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGES\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "RGBimg1 = io.imread('images/phone1.png')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "RGBimg2 = io.imread('images/phone2.png')\n",
    "img2 = img_as_float(color.rgb2gray(RGBimg2))\n",
    "\n",
    "# FEATURE DETECTION\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1, sigma = 1), threshold_rel=0.1)\n",
    "corners2 = feature.corner_peaks(feature.corner_shi_tomasi(img2, sigma = 1), threshold_rel=0.1)\n",
    "\n",
    "# FEATURE DESCRIPTION\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patches2 = patch_descriptor(img2, corners2, 40)\n",
    "\n",
    "# PLOT THE RESULT\n",
    "plt.figure(figsize=(12,6))\n",
    "D, match = spectral_matching2(0.5, patches1, patches2, corners1, corners2)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1, cmap=cm.gist_gray)\n",
    "plt.scatter(corners1[:,1], corners1[:,0], s=30)\n",
    "plt.title('skimage.feature.corner_peaks result')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2, cmap=cm.gist_gray)\n",
    "plt.scatter(corners2[:,1], corners2[:,0], s=30, c='r')\n",
    "plt.title('skimage.feature.corner_peaks result');\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,corners2,img1, img2)\n",
    "plt.subplots()\n",
    "plt.imshow(D)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corners 1:\" + str(corners1.shape))\n",
    "print(\"Corners 2:\" + str(corners2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** as I expected, using sigma = 1 corresdpond to find more corners respect to the sigma = 4 case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Translation\n",
    "We can see that the algorithm we applied is resistant to translation. This is expected because the patch descriptor we used don't change with translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGE\n",
    "RGBimg1 = io.imread('images/phone1.png')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "# TRANSLATION OF THE IMAGE\n",
    "import skimage.transform as tf\n",
    "small = tf.warp(img1, tf.SimilarityTransform(translation=(30, 0)))\n",
    "big = tf.warp(img1, tf.SimilarityTransform(translation=(180, 100)))\n",
    "\n",
    "# FEATURE DETECTION\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.1)\n",
    "cornersSmall = feature.corner_peaks(feature.corner_shi_tomasi(small), threshold_rel=0.1)\n",
    "cornersBig = feature.corner_peaks(feature.corner_shi_tomasi(big), threshold_rel=0.1)\n",
    "#print(corners1.shape, corners2.shape)\n",
    "\n",
    "# FEATURE DESCRIPTION\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patchesSmall = patch_descriptor(small, cornersSmall, 40)\n",
    "patchesBig = patch_descriptor(big, cornersBig, 40)\n",
    "\n",
    "# PLOT THE RESULT\n",
    "D, match = spectral_matching2(0.5, patches1, patchesSmall, corners1, cornersSmall)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersSmall,img1, small)\n",
    "\n",
    "\n",
    "D, match = spectral_matching2(0.5,patches1, patchesBig, corners1, cornersBig)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersBig,img1, big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Rotation\n",
    "- For a small rotation, the matching are still good.\n",
    "- For a very significant rotation (90°), very few corners have a match, and those which have are wrong. \n",
    "- For an even more big rotation (180°), we see that some corners on the numpad of the phone have the correct match, but the ones on the cup don't. This may be due to the symmetrical nature of those corners, which the one on the cup don't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGE\n",
    "RGBimg1 = io.imread('images/phone1.png')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "# ROTATION OF THE IMAGE\n",
    "import skimage.transform as tf\n",
    "small = tf.warp(img1, tf.SimilarityTransform(rotation = np.pi / 15))\n",
    "big = tf.warp(img1, tf.SimilarityTransform(rotation = np.pi/2, translation=(img1.shape[0], 0 )))\n",
    "veryBig = tf.warp(img1, tf.SimilarityTransform(rotation = np.pi, translation=(img1.shape[1], img1.shape[0] )))\n",
    "\n",
    "# FEATURE DETECTION\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.1)\n",
    "cornersSmall = feature.corner_peaks(feature.corner_shi_tomasi(small), threshold_rel=0.1)\n",
    "cornersBig = feature.corner_peaks(feature.corner_shi_tomasi(big), threshold_rel=0.1)\n",
    "cornersVBig = feature.corner_peaks(feature.corner_shi_tomasi(veryBig), threshold_rel=0.1)\n",
    "#print(corners1.shape, corners2.shape)\n",
    "\n",
    "# FEATURE DESCRIPTION\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patchesSmall = patch_descriptor(small, cornersSmall, 40)\n",
    "patchesBig = patch_descriptor(big, cornersBig, 40)\n",
    "patchesVBig = patch_descriptor(veryBig, cornersVBig, 40)\n",
    "\n",
    "# PLOT THE RESULT\n",
    "D,match = spectral_matching2(0.5,patches1, patchesSmall, corners1, cornersSmall)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersSmall,img1, small)\n",
    "\n",
    "D,match = spectral_matching2(0.5,patches1, patchesBig, corners1, cornersBig)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersBig,img1, big)\n",
    "\n",
    "D,match = spectral_matching2(0.5,patches1, patchesVBig, corners1, cornersVBig)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersVBig,img1, veryBig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Zoom in\n",
    "For small scale variation we don't have problems, but for a large change we see that most of the corners don't have a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IMAGE\n",
    "RGBimg1 = io.imread('images/shrub_L.jpg')\n",
    "img1 = img_as_float(color.rgb2gray(RGBimg1))\n",
    "\n",
    "# ZOOM IN OF THE IMAGE\n",
    "small = tf.warp(img1, tf.SimilarityTransform(scale = 0.9))\n",
    "big = tf.warp(img1, tf.SimilarityTransform(scale = 4))\n",
    "\n",
    "# FEATURE DETECTION\n",
    "corners1 = feature.corner_peaks(feature.corner_shi_tomasi(img1), threshold_rel=0.1)\n",
    "cornersSmall = feature.corner_peaks(feature.corner_shi_tomasi(small), threshold_rel=0.1)\n",
    "cornersBig = feature.corner_peaks(feature.corner_shi_tomasi(big), threshold_rel=0.1)\n",
    "#print(corners1.shape, corners2.shape)\n",
    "\n",
    "# FEATURE DESCRIPTION\n",
    "patches1 = patch_descriptor(img1, corners1, 40)\n",
    "patchesSmall = patch_descriptor(small, cornersSmall, 40)\n",
    "patchesBig = patch_descriptor(big, cornersBig, 40)\n",
    "\n",
    "# PLOT THE RESULT\n",
    "D,match = spectral_matching2(0.5,patches1, patchesSmall, corners1, cornersSmall)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersSmall,img1, small)\n",
    "\n",
    "D,match = spectral_matching2(0.5,patches1, patchesBig, corners1, cornersBig)\n",
    "plt.figure(figsize=(12, 6))\n",
    "match_euclidean = show_match2(match,corners1,cornersBig,img1, big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add your final discussion here !\n",
    "\n",
    "From the experiments we can assert that the choice of the patch is really important. \n",
    "\n",
    "In the case of translation, we have seen that this type of transformation doesn't affect the correctness of the matching procedure. **But** in the case of rotation and scale the correctess of the matching procedure is affected in a negative way in fact we have seen that we get worse result respect to the translation case. \n",
    "\n",
    "$\\xrightarrow{}$ So I think that if we had some prior informations on what kind of change we could expect to match, we could make a more apt choiche, getting a better result!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
